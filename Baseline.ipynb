{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "96ffT8RObDmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "DLYxhylcBmUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "WlVOLbIr-ZdA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6lcWRW-6TQH"
      },
      "outputs": [],
      "source": [
        "# !wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "# !tar xvf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\"\n",
        "rawData = open(path)\n",
        "reader = csv.reader(rawData, delimiter=\"\\t\")\n",
        "dataset = []"
      ],
      "metadata": {
        "id": "CGkWhEdl7kHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = []\n",
        "pair = []\n",
        "sent = []\n",
        "i = 0\n",
        "rowno = 1\n",
        "for row in reader:\n",
        "\n",
        "    if row[0] == \"</s>\":\n",
        "        dataset.append(sent)\n",
        "        sent = []\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        pair.append(row[0])\n",
        "        pair.append(row[1])\n",
        "        sent.append(pair)\n",
        "        pair = []\n",
        "    except IndexError:\n",
        "        error.append(rowno)\n",
        "\n",
        "    rowno += 1"
      ],
      "metadata": {
        "id": "27ANyeUz9ssd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[:6000]\n",
        "dev_data = dataset[6000:8000]\n",
        "test_data = dataset[8000:]"
      ],
      "metadata": {
        "id": "8XBWedH49xGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Model"
      ],
      "metadata": {
        "id": "Mb5lQYp3CzR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "QtPPuP3kbVC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_size):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, src_size, enforce_sorted=False\n",
        "        )\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "IvC6Ycn198Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "rahy_1yCbWjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(self.hid_dim, emb_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(emb_dim, self.output_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, context):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = output.view(-1, output.size(2))\n",
        "        output = self.fc_out(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "WButlvpXC1jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Connection"
      ],
      "metadata": {
        "id": "rUspwflubZWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, src_size, teacher_forcing_ratio=0.5):\n",
        "        batch_size = tgt.shape[0]\n",
        "\n",
        "        encoder_output, encoder_hidden = self.encoder(src, src_size)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        outputs = torch.zeros(batch_size, self.decoder.output_dim, tgt.size(1)).to(\n",
        "            self.device\n",
        "        )\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
        "        outputs[:, 1, 0] = 1\n",
        "\n",
        "        for t in range(1, tgt.size(1)):\n",
        "            decoder_output, decoder_hidden = self.decoder(\n",
        "                decoder_input, decoder_hidden, encoder_output\n",
        "            )\n",
        "            outputs[:, :, t] = decoder_output\n",
        "            top1 = decoder_output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            decoder_input = (\n",
        "                tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
        "            )\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "kUHyTOnEC2Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Initialization"
      ],
      "metadata": {
        "id": "MDYAjOrnbtk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)"
      ],
      "metadata": {
        "id": "zH2616MOnkdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "b85Qy1ZHCat9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_lower_script = [chr(alpha) for alpha in range(97, 123)]\n",
        "devanagari_script = [\n",
        "    \"ऄ\",\n",
        "    \"अ\",\n",
        "    \"आ\",\n",
        "    \"इ\",\n",
        "    \"ई\",\n",
        "    \"उ\",\n",
        "    \"ऊ\",\n",
        "    \"ऍ\",\n",
        "    \"ऎ\",\n",
        "    \"ए\",\n",
        "    \"ऐ\",\n",
        "    \"ऑ\",\n",
        "    \"ऒ\",\n",
        "    \"ओ\",\n",
        "    \"औ\",\n",
        "    \"ऋ\",\n",
        "    \"ॠ\",\n",
        "    \"ऌ\",\n",
        "    \"ॡ\",\n",
        "    \"ॲ\",\n",
        "    \"ॐ\",\n",
        "    \"क\",\n",
        "    \"ख\",\n",
        "    \"ग\",\n",
        "    \"घ\",\n",
        "    \"ङ\",\n",
        "    \"च\",\n",
        "    \"छ\",\n",
        "    \"ज\",\n",
        "    \"झ\",\n",
        "    \"ञ\",\n",
        "    \"ट\",\n",
        "    \"ठ\",\n",
        "    \"ड\",\n",
        "    \"ढ\",\n",
        "    \"ण\",\n",
        "    \"त\",\n",
        "    \"थ\",\n",
        "    \"द\",\n",
        "    \"ध\",\n",
        "    \"न\",\n",
        "    \"ऩ\",\n",
        "    \"प\",\n",
        "    \"फ\",\n",
        "    \"ब\",\n",
        "    \"भ\",\n",
        "    \"म\",\n",
        "    \"य\",\n",
        "    \"र\",\n",
        "    \"ऱ\",\n",
        "    \"ल\",\n",
        "    \"ळ\",\n",
        "    \"ऴ\",\n",
        "    \"व\",\n",
        "    \"श\",\n",
        "    \"ष\",\n",
        "    \"स\",\n",
        "    \"ह\",\n",
        "    \"क़\",\n",
        "    \"ख़\",\n",
        "    \"ग़\",\n",
        "    \"ज़\",\n",
        "    \"ड़\",\n",
        "    \"ढ़\",\n",
        "    \"फ़\",\n",
        "    \"य़\",\n",
        "    \"्\",\n",
        "    \"ा\",\n",
        "    \"ि\",\n",
        "    \"ी\",\n",
        "    \"ु\",\n",
        "    \"ू\",\n",
        "    \"ॅ\",\n",
        "    \"ॆ\",\n",
        "    \"े\",\n",
        "    \"ै\",\n",
        "    \"ॉ\",\n",
        "    \"ॊ\",\n",
        "    \"ो\",\n",
        "    \"ौ\",\n",
        "    \"ृ\",\n",
        "    \"ॄ\",\n",
        "    \"ॢ\",\n",
        "    \"ॣ\",\n",
        "    \"ँ\",\n",
        "    \"ं\",\n",
        "    \"ः\",\n",
        "    \"़\",\n",
        "    \"॑\",\n",
        "    \"ऽ\",\n",
        "    chr(0x200C),\n",
        "    chr(0x200D),\n",
        "]"
      ],
      "metadata": {
        "id": "0dJz_LhKCAfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Script:\n",
        "    def __init__(self, language_script=devanagari_script):\n",
        "        self.graphemes = language_script\n",
        "        self.char2index = {}\n",
        "        self.index2char = {}\n",
        "        self.char2index[\"_\"] = 0\n",
        "        self.char2index[\"^\"] = 1\n",
        "        self.char2index[\"$\"] = 2\n",
        "        self.index2char[0] = \"_\"\n",
        "        self.index2char[1] = \"^\"\n",
        "        self.index2char[2] = \"$\"\n",
        "\n",
        "        for index, char in enumerate(self.graphemes):\n",
        "            self.char2index[char] = index + 3\n",
        "            self.index2char[index + 3] = char\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.char2index)\n",
        "\n",
        "    def word2vector(self, word):\n",
        "        vector = list()\n",
        "        vector.append(self.char2index[\"^\"])\n",
        "        for char in list(word):\n",
        "            if char in self.char2index:\n",
        "                vector.append(self.char2index[char])\n",
        "        vector.append(self.char2index[\"$\"])\n",
        "        vector = np.asarray(vector, dtype=np.int64)\n",
        "        return vector\n",
        "\n",
        "    def vector2word(self, vector):\n",
        "        word = list()\n",
        "        for index in vector:\n",
        "            word.append(self.index2char[index])\n",
        "        word = \"\".join(word).replace(\"_\", \"\").replace(\"^\", \"\").replace(\"$\", \"\")\n",
        "        return word"
      ],
      "metadata": {
        "id": "Cn-AvT06ChEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transliteration_Dataset(Dataset):\n",
        "    def __init__(self, data, src_script, tgt_script):\n",
        "        src_data = list()\n",
        "        tgt_data = list()\n",
        "        for sentence in data:\n",
        "            src, tgt = zip(*sentence)\n",
        "            for i in range(len(src)):\n",
        "                flag = 0\n",
        "                for src_char in src[i]:\n",
        "                    for tgt_char in tgt[i]:\n",
        "                        if (\n",
        "                            src_char not in src_script.graphemes\n",
        "                            or tgt_char not in tgt_script.graphemes\n",
        "                        ):\n",
        "                            flag = 1\n",
        "                            break\n",
        "                    if flag == 1:\n",
        "                        break\n",
        "                if flag == 0:\n",
        "                    src_data.append(src[i])\n",
        "                    tgt_data.append(tgt[i])\n",
        "\n",
        "        self.src_sript = src_script\n",
        "        self.tgt_sript = tgt_script\n",
        "        self.src = [src_script.word2vector(word) for word in src_data]\n",
        "        self.tgt = [tgt_script.word2vector(word) for word in tgt_data]\n",
        "        self.max_src_size = max([len(vector) for vector in self.src], default=0)\n",
        "        self.max_tgt_size = max([len(vector) for vector in self.tgt], default=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_vector = self.pad_sequence(self.src[index], self.max_src_size)\n",
        "        tgt_vector = self.pad_sequence(self.tgt[index], self.max_tgt_size)\n",
        "        return src_vector, tgt_vector, len(self.src[index])\n",
        "\n",
        "    def pad_sequence(self, vector, max_size):\n",
        "        padded_vector = np.zeros((max_size), dtype=np.int64)\n",
        "        if len(vector) > max_size:\n",
        "            padded_vector[:] = vector[:max_size]\n",
        "        else:\n",
        "            padded_vector[: len(vector)] = vector\n",
        "        return padded_vector"
      ],
      "metadata": {
        "id": "2YW3ffURNDyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "iB-uQcGrnFSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation"
      ],
      "metadata": {
        "id": "dDA9y-Z4czsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_script = Script(devanagari_script)\n",
        "tgt_script = Script(english_lower_script)\n",
        "\n",
        "train_dataset = Transliteration_Dataset(train_data, src_script, tgt_script)\n",
        "dev_dataset = Transliteration_Dataset(dev_data, src_script, tgt_script)\n",
        "test_dataset = Transliteration_Dataset(test_data, src_script, tgt_script)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "QSG2KAMvpt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters & Model Architecture"
      ],
      "metadata": {
        "id": "thV0zbamccss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "INPUT_DIM = src_script.size()\n",
        "OUTPUT_DIM = tgt_script.size()\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5dn1wCjNEvYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "fA18DnxDLqDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation"
      ],
      "metadata": {
        "id": "YtEsynLUdJsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "EUrLsJts-Vqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization Handling"
      ],
      "metadata": {
        "id": "G-eDAu03dOhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "CLxj6fjyE9Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(pred_tensor, tgt_tensor, script):\n",
        "    pred_sequence = torch.argmax(pred_tensor, dim=1)\n",
        "    batch_size = pred_sequence.shape[0]\n",
        "    count = 0\n",
        "    for i in range(batch_size):\n",
        "        pred = script.vector2word(pred_sequence[i, :].cpu().numpy())\n",
        "        tgt = script.vector2word(tgt_tensor[i, :].cpu().numpy())\n",
        "        if pred == tgt:\n",
        "            count += 1\n",
        "    return torch.tensor(count / batch_size)"
      ],
      "metadata": {
        "id": "k-_InD5Z3gG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(pred, tgt):\n",
        "    mask = tgt.ge(1).type(torch.FloatTensor).to(device)\n",
        "    loss = criterion(pred, tgt) * mask\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "6-4kteMx4ET2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "-b2V9VU4d9Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, tgt, src_size) in enumerate(iterator):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt, src_size)\n",
        "        loss = calculate_loss(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "M2wFFO5lFDSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "h5WdH-q4eBdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt, src_size) in enumerate(iterator):\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            output = model(src, tgt, src_size, 0)\n",
        "            loss = calculate_loss(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += calculate_accuracy(output, tgt, tgt_script)\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
      ],
      "metadata": {
        "id": "3R-YlIY_FGXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "eF_H05myeC72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_dev_loss = float(\"inf\")\n",
        "best_dev_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, 1)\n",
        "    dev_loss, dev_accuracy = evaluate(model, dev_dataloader)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    epoch_mins = int(epoch_time / 60)\n",
        "    epoch_secs = int(epoch_time - (epoch_mins * 60))\n",
        "\n",
        "    if dev_loss < best_dev_loss:\n",
        "        best_dev_loss = dev_loss\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        torch.save(model.state_dict(), \"baseline.pt\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | Train Loss: {train_loss:.3f} | Val. Loss: {dev_loss:.3f} | Val. Acc: {dev_accuracy:.3f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "VpYQSE3xFOKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Dataset"
      ],
      "metadata": {
        "id": "KujgqymBeKqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"baseline.pt\"))\n",
        "\n",
        "test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "x4Bwr16DFPF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}